{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import openai\n",
    "import argparse\n",
    "import os\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import AgentType\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "import openai\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import shutil\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "embedding = OpenAIEmbeddings()\n",
    "import re\n",
    "from paperqa import Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['volume_per_atom_0', 'symmetry_function_G1_19', 'unoccupied_energy_levels_CBM_4', 'distance_matrix_3', 'symmetry_function_G1_14']\n"
     ]
    }
   ],
   "source": [
    "top_shap = list(np.load(f'./data/top_shap_features.npy',allow_pickle=True))\n",
    "print(top_shap\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'paperscraper' has no attribute 'search_papers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m keyword_search \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmetal organic frameworks open metal sites\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m papers \u001b[39m=\u001b[39m paperscraper\u001b[39m.\u001b[39;49msearch_papers(keyword_search, limit\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'paperscraper' has no attribute 'search_papers'"
     ]
    }
   ],
   "source": [
    "keyword_search = 'metal organic frameworks open metal sites'\n",
    "papers = paperscraper.search_papers(keyword_search, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_directory = './data/arxiv_downloads/'\n",
    "docs_list = [f'{lit_directory}/{doc}' for doc in os.listdir(lit_directory)]\n",
    "qa_docs = Docs()\n",
    "for d in docs_list:\n",
    "    qa_docs.add(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m answer \u001b[39m=\u001b[39m qa_docs\u001b[39m.\u001b[39;49mquery(\u001b[39m\"\u001b[39;49m\u001b[39mWhat is a MOF?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/shap/lib/python3.11/site-packages/paperqa/docs.py:577\u001b[0m, in \u001b[0;36mDocs.query\u001b[0;34m(self, query, k, max_sources, length_prompt, marginal_relevance, answer, key_filter, get_callbacks)\u001b[0m\n\u001b[1;32m    575\u001b[0m     loop \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mnew_event_loop()\n\u001b[1;32m    576\u001b[0m     asyncio\u001b[39m.\u001b[39mset_event_loop(loop)\n\u001b[0;32m--> 577\u001b[0m \u001b[39mreturn\u001b[39;00m loop\u001b[39m.\u001b[39;49mrun_until_complete(\n\u001b[1;32m    578\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maquery(\n\u001b[1;32m    579\u001b[0m         query,\n\u001b[1;32m    580\u001b[0m         k\u001b[39m=\u001b[39;49mk,\n\u001b[1;32m    581\u001b[0m         max_sources\u001b[39m=\u001b[39;49mmax_sources,\n\u001b[1;32m    582\u001b[0m         length_prompt\u001b[39m=\u001b[39;49mlength_prompt,\n\u001b[1;32m    583\u001b[0m         marginal_relevance\u001b[39m=\u001b[39;49mmarginal_relevance,\n\u001b[1;32m    584\u001b[0m         answer\u001b[39m=\u001b[39;49manswer,\n\u001b[1;32m    585\u001b[0m         key_filter\u001b[39m=\u001b[39;49mkey_filter,\n\u001b[1;32m    586\u001b[0m         get_callbacks\u001b[39m=\u001b[39;49mget_callbacks,\n\u001b[1;32m    587\u001b[0m     )\n\u001b[1;32m    588\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/shap/lib/python3.11/asyncio/base_events.py:629\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \n\u001b[1;32m    620\u001b[0m \u001b[39mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[39mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[0;32m--> 629\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_running()\n\u001b[1;32m    631\u001b[0m new_task \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m futures\u001b[39m.\u001b[39misfuture(future)\n\u001b[1;32m    632\u001b[0m future \u001b[39m=\u001b[39m tasks\u001b[39m.\u001b[39mensure_future(future, loop\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/shap/lib/python3.11/asyncio/base_events.py:588\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_running\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    587\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_running():\n\u001b[0;32m--> 588\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThis event loop is already running\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    589\u001b[0m     \u001b[39mif\u001b[39;00m events\u001b[39m.\u001b[39m_get_running_loop() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    590\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    591\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mCannot run the event loop while another loop is running\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "answer = qa_docs.query(\"What is a MOF?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "            temperature=0,\n",
    "            model_name=\"gpt-3.5-turbo-0613\",\n",
    "            request_timeout=1000)\n",
    "    \n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "readonlymemory = ReadOnlySharedMemory(memory=memory)\n",
    "\n",
    "db1 = Chroma(persist_directory=\"./data/chroma/\", \n",
    "                    embedding_function=embedding)\n",
    "retriever1 = db1.as_retriever()\n",
    "chain1 =  ConversationalRetrievalChain.from_llm(llm, \n",
    "                                                retriever=retriever1, \n",
    "                                                memory=readonlymemory,\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_shap = list(np.load(f'./data/top_shap_features.npy',allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC of the model is 0.876713103626608.\n",
      "The AUC of the model is 0.876713103626608.\n",
      "The AUC of the model is 0.876713103626608.\n"
     ]
    }
   ],
   "source": [
    "for ft in top_shap:\n",
    "    feature = ' '.join(ft.split('_')[:-1])\n",
    "    query = f\"\"\"What is the AUC of the model?\n",
    "    \n",
    "        \"\"\"\n",
    "\n",
    "    response = chain1.run(query)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no mention of AUC' in the provided context. Therefore, I don't have enough information to answer your question.\n"
     ]
    }
   ],
   "source": [
    "response = chain1.run(\"what is the AUC'?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _explain_lime(data_path,label,model_path,\n",
    "                 top_k=5,savedir='./data',\n",
    "                 mode='classification',num_samples=100):\n",
    "   \n",
    "   weights = []\n",
    "\n",
    "   df = pd.read_csv(data_path,header=0)\n",
    "   ## use all data for the lime analysis\n",
    "   df_x = df.drop(label,axis = 1)\n",
    "\n",
    "   model = xgb.XGBClassifier()\n",
    "   model.load_model(model_path)\n",
    "\n",
    "   if mode=='classification': class_names=[0,1]\n",
    "   else: class_names=[f'{label}']\n",
    "\n",
    "   explainer = LimeTabularExplainer(df_x.values, \n",
    "                                    feature_names=list(df_x.columns), \n",
    "                                    class_names=class_names,\n",
    "                                    mode=mode)\n",
    "   \n",
    "\n",
    "   df_sample = df_x.sample(num_samples)\n",
    "   for i in range(len(df_sample)):\n",
    "        exp = explainer.explain_instance(df_sample.iloc[i], model.predict_proba, \n",
    "        num_features=700, top_labels=True)\n",
    "        exp_map = exp.as_map()\n",
    "        cls = list(exp_map.keys())[0]\n",
    "        #sort weights otherwise they are printed from highest to lowest w\n",
    "        exp_map[cls].sort(key=lambda x: x[0])\n",
    "        ws = np.array(exp_map[cls])[:,-1]\n",
    "        zscore = stats.zscore(ws)\n",
    "        weights.append(zscore)\n",
    "        \n",
    "   global_w = np.sum(np.vstack(weights),axis=0)\n",
    "\n",
    "   ## get top k features from LIME values\n",
    "   top_fts = np.array(df_x.columns)[abs(global_w).argsort()[-top_k:][::-1]]\n",
    "   lime = global_w[abs(global_w).argsort()[-top_k:][::-1]]\n",
    "   y_pos = np.arange(top_k)\n",
    "\n",
    "   ## plot LIME analysis \n",
    "   fig, ax = plt.subplots()\n",
    "   ax.barh(y_pos, lime)\n",
    "   ax.set_yticks(y_pos, labels=top_fts )\n",
    "   ax.invert_yaxis()  # labels read top-to-bottom\n",
    "   ax.set_xlabel('Z-score lime values')\n",
    "   ax.set_ylabel('Features')\n",
    "   fig.savefig(f'{savedir}/lime_bar_top_{top_k}.png',\n",
    "               bbox_inches='tight', dpi=300)\n",
    "   \n",
    "   ## write summary of LIME analysis\n",
    "   summary = f'To explain the model behavior, LIME explanations were generated. \\\n",
    "    Please note these are global observations based on {num_samples} data points.'\n",
    "   \n",
    "   for ft, l in zip(top_fts,lime):\n",
    "        summary+= f\"Feature {ft} has an average \\\n",
    "             z-score of {l} with towards the prediction.\"\n",
    "        \n",
    "   prompt = f\"Summarize and write an brief of the model behavior \\\n",
    "    from the following: {summary}\"\n",
    "   lime_summary = get_response(prompt)\n",
    "\n",
    "   #save summary\n",
    "   with open(f'{savedir}/lime_summary.txt',\"w+\") as f: \n",
    "       f.write(lime_summary)\n",
    "       f.close()\n",
    "\n",
    "   #update vectordb\n",
    "   vector_db(lit_directory = f'{savedir}/lime_summary.txt')\n",
    "\n",
    "   np.save(f'{savedir}/top_{top_k}_lime_fts.npy',top_fts)\n",
    "\n",
    "   return top_fts\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
